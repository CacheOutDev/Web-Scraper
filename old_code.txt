'''
def findNavLinks():
    department_url = re.split(r'/', department_address)
    department_url = department_url.pop(3)
    parent_patterns = re.compile(r'/'+ department_url +'/')
    parent_links = soup.find_all(href=parent_patterns)

    for parent_link in parent_links:
        parent_pages.append(parent_link.get('href'))


    print(parent_pages)

findNavLinks()
'''
__________________________________________
    
    nav_links = soup.find_all(class_="nav-link")
    #print(nav_links)

    link_patterns = re.compile(r'/financial-support-services')
    relevant_links = soup.find_all(href=link_patterns)

    for nav_link in nav_links:
        if "/financial-support-services" in nav_link:
            parent_pages.append(nav_link.get('href'))

        #if department_address in nav_link:
            #parent_pages.append(nav_link.get('href'))
        #else:
            #pass

    print(parent_pages)

__________________________________________
page = requests.get(department_address)
soup = BeautifulSoup(page.content, "html.parser")

parent_pages, child_pages = [], []
parent_pages = []
# child_pages = []
link_name, link_address, migration_status, deletion_status = [], [], [], []

def findNavLinks():
    # navBar_links = soup.find_all(class_="nav navbar-nav")
    # navItem_links = nav_links.find_all(class_="nav-item")
    # after cutting down size of links, look for links that match the re pattern
    department_url = re.split(r'/', department_address)
    department_url = department_url.pop(3)
    parent_patterns = re.compile(r'/'+ department_url +'/')
    parent_links = soup.find_all(href=parent_patterns)

    for parent_link in parent_links:
        parent_pages.append(parent_link.get('href'))
        
    print(parent_pages)

    href_pattern = "https://csustan.edu"
    newhrefpattern = department_address.slice(href_pattern)
    print(newhrefpattern)
    #navBar_pattern = re.compile(href_pattern)
    #navBar_links = soup.find_all(href=navBar_pattern) 
    #print(navBar_links) 
'''

link_name, link_address, migration_status, deletion_status = [], [], [], []

link_patterns = re.compile(r'sites|StanStatePublicDocs')
relevant_links = soup.find_all(href=link_patterns)

    for navBar_link in navBar_links:
        navBar_link = navBar_link.get('href')
        navBar_link = "https://www.csustan.edu" + navBar_link
        parent_pages.append(navBar_link)
def addColumnValues():
    link_patterns = re.compile(r'sites|StanStatePublicDocs')
    relevant_links = soup.find_all(href=link_patterns)

    if len(relevant_links) != 0:
        for relevant_link in relevant_links:
            link_name.append(relevant_link.get_text())
            link_address.append(relevant_link.get('href'))
            migration_status.append(' ')
            deletion_status.append(' ')
            parent_pages.append(' ') # remove once pagination is set
            child_pages.append(' ') # remove once pagination is set
    else:
        link_name.append('No links present on this page.')
        link_address.append(' ')
        migration_status.append(' ')
        deletion_status.append(' ')
        parent_pages.append(' ') # remove once pagination is set
        child_pages.append(' ') # remove once pagination is set
        print("No links present on this page.")

def createDataFrame():
    df = pd.DataFrame(parent_pages, columns = ['Parent Pages'])
    df['Child Pages'] = child_pages
    # df['Child Pages'] = child_pages
    df['Link Name'] = link_name
    df['Link Address'] = link_address
    df['Migrated to SP'] = migration_status
@@ -62,9 +54,11 @@ def createDataFrame():
    excel_name = excel_html.get_text()

    df = df.to_excel(excel_name + ".xlsx")
    '''

addColumnValues()
createDataFrame()
'''
findNavLinks()

#for parent_page in parent_pages:
    #addColumnValues()

findNavLinks()
#createDataFrame()